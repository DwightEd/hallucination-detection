# =============================================================================
# Feature Extraction Configuration - Memory Optimized Default
# =============================================================================
# 默认配置，针对内存优化
# 适用于大多数检测方法（lapeigvals, entropy, lookback_lens等）
#
# ⚠️ 此配置不存储完整注意力矩阵，以防止OOM
# 如需完整注意力，请使用 with_full_attentions.yaml
# =============================================================================

# -----------------------------------------------------------------------------
# 提取模式
# teacher_forcing: 使用真实响应计算特征（推荐）
# generation: 先生成响应再提取特征
# -----------------------------------------------------------------------------
mode: teacher_forcing

# -----------------------------------------------------------------------------
# 存储的特征类型（用于记录）
# -----------------------------------------------------------------------------
stored_features: attention_diags

# =============================================================================
# 注意力特征
# =============================================================================

# 是否提取注意力特征
attention_enabled: true

# 提取哪些层的注意力
# all: 所有层
# last_n:4: 最后4层
# 0,1,2,3: 指定层索引
# -4,-3,-2,-1: 负数索引（最后4层）
attention_layers: all

# 注意力存储方式
# diagonal: 仅对角线（内存友好）
# full: 完整矩阵（高内存）
attention_storage: diagonal

# -----------------------------------------------------------------------------
# ⚠️ 完整注意力存储 (CRITICAL)
# 
# 设为 true 会消耗大量显存！
# 
# 内存估算 (float16):
#   seq_len=2048, n_layers=32, n_heads=32:
#   2048 * 2048 * 32 * 32 * 2 bytes ≈ 17 GB
#
# 建议：
#   - 24GB显存: max_length <= 4096
#   - 16GB显存: max_length <= 2048
#   - 8GB显存:  max_length <= 1024
#
# 仅在使用 hypergraph 方法时需要设为 true
# -----------------------------------------------------------------------------
store_full_attention: false

# =============================================================================
# 隐藏状态特征
# =============================================================================

# 是否提取隐藏状态
hidden_states_enabled: true

# 提取哪些层的隐藏状态
# last: 仅最后一层
# last_n:4: 最后4层
# all: 所有层（高内存）
# -4,-3,-2,-1: 负数索引
hidden_states_layers: last_n:4

# 隐藏状态池化方式
# last_token: 使用最后一个token（推荐）
# mean: 平均池化
# max: 最大池化
# cls: 使用[CLS] token（如果存在）
hidden_states_pooling: last_token

# =============================================================================
# Token概率特征
# =============================================================================

# 是否提取token概率
token_probs_enabled: true

# 保存top-k概率
# 用于计算熵等统计量
token_probs_top_k: 10

# =============================================================================
# 长度和批次限制
# =============================================================================

# 最大序列长度
# ⚠️ 这是token长度限制，不是字符长度
# 超过此长度的序列会被截断（从末尾截断）
# 
# GPU内存参考 (不含full_attention):
#   - H100 (80GB): max_length <= 16384
#   - A100 (80GB): max_length <= 16384  
#   - A100 (40GB): max_length <= 8192
#   - RTX 4090 (24GB): max_length <= 4096
#   - RTX 3090 (24GB): max_length <= 4096
#   - V100 (16GB): max_length <= 2048
#
# 建议: 如果遇到 "prompt > max_length" 的警告，请增加此值
max_length: 8192

# 批次大小
# 建议设为1-4以平衡内存和吞吐量
# 对于H100等大显存GPU，可以设为4-8
batch_size: 4

# =============================================================================
# 高级选项
# =============================================================================

# 是否使用混合精度
use_fp16: true

# 是否在每个样本后清理GPU缓存
clear_cache_per_sample: true

# 检查点保存间隔（样本数）
checkpoint_interval: 100
