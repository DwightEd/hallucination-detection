# =============================================================================
# LapEigvals Features Configuration
# =============================================================================
# 关键: LapEigvals 需要完整注意力矩阵来计算正确的Laplacian特征值
# 
# 论文公式:
# - d_ii = Σ_{u>i} a_ui / (T-i)  需要访问 attention[u, i] for all u > i
# - λ_i = d_ii - a_ii
# =============================================================================

mode: teacher_forcing

# =============================================================================
# 注意力配置 - 关键修改
# =============================================================================
attention_enabled: true
attention_layers: all

# 选项1: 存储完整注意力矩阵 (内存密集，但最准确)
# 适用于较短序列或有足够内存的情况
store_full_attention: true      # 启用完整注意力存储
attention_storage: full         # 存储完整矩阵

# 选项2 (备选): 在提取时计算laplacian_diags
# 如果内存不足，可以设置:
# store_full_attention: false
# compute_laplacian_diags: true  # 在提取时即时计算并存储laplacian对角线

# =============================================================================
# 其他特征 (LapEigvals不需要)
# =============================================================================
hidden_states_enabled: false
token_probs_enabled: false

# =============================================================================
# 序列和批处理设置
# =============================================================================
max_length: 2048               # 较短以减少内存
batch_size: 1                  # 单样本处理以控制内存

# =============================================================================
# 内存优化选项
# =============================================================================
# 如果内存仍然不足，考虑:
# 1. 减少 max_length
# 2. 使用 compute_laplacian_diags: true 并设置 store_full_attention: false
# 3. 分层处理: 一次只处理部分层的注意力
# =============================================================================
