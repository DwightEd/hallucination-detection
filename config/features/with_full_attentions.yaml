# =============================================================================
# Feature Extraction Configuration - With Full Attention Matrices
# =============================================================================
# ⚠️ 警告：此配置需要大量GPU显存！
# 
# 仅在以下情况使用此配置：
# 1. 使用 hypergraph 方法（需要完整注意力矩阵构建超图）
# 2. 有足够的GPU显存（建议 A100 80GB 或 2x RTX 4090）
#
# 内存估算 (seq_len=2048, n_layers=32, n_heads=32, float16):
#   完整注意力: 2048 * 2048 * 32 * 32 * 2 bytes ≈ 17 GB/样本
#   加上模型权重和中间状态，总共需要 ~40-50 GB
#
# 如果遇到OOM，请：
# 1. 减小 max_length
# 2. 使用 default.yaml 代替
# 3. 使用更大显存的GPU
# =============================================================================

# -----------------------------------------------------------------------------
# 基本设置
# -----------------------------------------------------------------------------
mode: teacher_forcing

# ⚠️ 减小的max_length以适应完整注意力存储
# 如果仍然OOM，尝试进一步减小到1024
max_length: 2048

# -----------------------------------------------------------------------------
# 批次大小
# 必须设为1，因为完整注意力矩阵太大
# -----------------------------------------------------------------------------
batch_size: 16

# =============================================================================
# 注意力特征
# =============================================================================

attention_enabled: true

# hypergraph方法需要所有层的注意力
attention_layers: all

# -----------------------------------------------------------------------------
# ⚠️ 完整注意力存储 - 高内存消耗
# 
# 设为 true 时：
# - 存储形状为 [n_layers, n_heads, seq_len, seq_len] 的张量
# - 用于构建注意力超图
# - 内存需求随 seq_len^2 增长！
#
# 硬件建议：
# - A100 80GB: 可处理 max_length=4096
# - RTX 4090 24GB: max_length <= 1536
# - RTX 3090 24GB: max_length <= 1536
# - 2x RTX 4090 (multi-GPU): max_length <= 2048
# -----------------------------------------------------------------------------
store_full_attention: true

# =============================================================================
# 隐藏状态特征
# =============================================================================

hidden_states_enabled: true

# 使用最后一层以节省内存
hidden_states_layers: last

# 池化方式
hidden_states_pooling: last_token

# =============================================================================
# Token概率特征
# =============================================================================

token_probs_enabled: true
token_probs_top_k: 10

# =============================================================================
# 高级内存优化选项
# =============================================================================

# 使用混合精度（强烈推荐）
use_fp16: true

# 每个样本后清理GPU缓存
clear_cache_per_sample: true

# 逐层处理（减少峰值内存）
process_layers_sequentially: true

# 检查点保存间隔（更频繁以防崩溃）
checkpoint_interval: 50

# =============================================================================
# 使用说明
# =============================================================================
# 
# 1. 确保全局配置中 allow_full_attention=true:
#    python scripts/generate_activations.py \
#        features=with_full_attentions \
#        allow_full_attention=true
#
# 2. 使用多GPU（如果可用）:
#    python scripts/generate_activations.py \
#        features=with_full_attentions \
#        model.multi_gpu.enabled=true
#
# 3. 如果OOM，尝试减小max_length:
#    python scripts/generate_activations.py \
#        features=with_full_attentions \
#        features.max_length=1024
#
# =============================================================================
