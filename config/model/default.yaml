# =============================================================================
# Model Configuration - Default Template
# =============================================================================
# 模型加载的默认配置模板
# 支持多GPU分布和量化加载
# =============================================================================

# -----------------------------------------------------------------------------
# 模型标识
# -----------------------------------------------------------------------------
# Hugging Face模型名称或本地路径（必填）
name: ???

# 输出路径使用的短名称
# null = 自动从name生成
short_name: ???

# -----------------------------------------------------------------------------
# 模型架构参数
# 这些值会在加载模型后自动更新
# -----------------------------------------------------------------------------
n_layers: 32        # 模型层数
n_heads: 32         # 注意力头数
hidden_size: 4096   # 隐藏层维度
context_size: 8192  # 上下文窗口大小

# -----------------------------------------------------------------------------
# 数据类型
# float16: 半精度（节省显存，可能有精度损失）
# bfloat16: Brain浮点（推荐，A100/RTX 40系支持）
# float32: 全精度（高内存，通常不需要）
# -----------------------------------------------------------------------------
dtype: bfloat16

# -----------------------------------------------------------------------------
# 设备映射
# auto: 自动分配到可用GPU
# cpu: 仅使用CPU
# cuda:0: 指定GPU
# -----------------------------------------------------------------------------
device_map: auto

# -----------------------------------------------------------------------------
# 其他加载选项
# -----------------------------------------------------------------------------
# 是否信任远程代码（某些模型需要）
trust_remote_code: true

# 注意力实现方式
# eager: 标准实现（必须使用此选项以提取注意力）
# flash_attention_2: FlashAttention（不支持注意力提取）
# sdpa: Scaled Dot Product Attention
attn_implementation: eager

# -----------------------------------------------------------------------------
# 量化选项
# 用于减少显存占用，适合显存有限的情况
# -----------------------------------------------------------------------------
# 4bit量化（最节省显存，可能影响精度）
load_in_4bit: false

# 8bit量化（平衡显存和精度）
load_in_8bit: false

# -----------------------------------------------------------------------------
# 分词器配置
# -----------------------------------------------------------------------------
tokenizer_name: ${model.name}
tokenizer_padding_side: left

# =============================================================================
# 多GPU配置
# =============================================================================
# 
# 当有多张GPU时，可以启用多GPU支持将模型分布到多卡
# 
# 使用方法：
# 1. 在命令行覆盖:
#    python scripts/generate_activations.py \
#        model.multi_gpu.enabled=true \
#        model.multi_gpu.strategy=auto
#
# 2. 在配置文件中启用（取消下面的注释）
#
# -----------------------------------------------------------------------------

# multi_gpu:
#   # 是否启用多GPU
#   enabled: false
#   
#   # 分布策略
#   # auto: transformers自动分配
#   # balanced: 均衡分配到所有GPU
#   # sequential: 按顺序填充GPU
#   # single: 仅使用单个GPU
#   strategy: auto
#   
#   # 每个GPU的最大显存限制
#   # 格式: {gpu_id: "显存大小"}
#   # 示例: 两张24GB卡各用20GB
#   max_memory:
#     0: "20GB"
#     1: "20GB"
#   
#   # 限制使用的GPU数量（null = 使用所有可用GPU）
#   num_gpus: null
#   
#   # 主设备（用于输入）
#   main_device: 0

# =============================================================================
# 多GPU配置示例
# =============================================================================
#
# 示例1: 2x RTX 4090 均衡分配
# multi_gpu:
#   enabled: true
#   strategy: balanced
#   max_memory:
#     0: "22GB"
#     1: "22GB"
#
# 示例2: A100 80GB + RTX 4090，主要使用A100
# multi_gpu:
#   enabled: true
#   strategy: auto
#   max_memory:
#     0: "75GB"  # A100
#     1: "20GB"  # RTX 4090
#
# 示例3: 仅使用第一张GPU
# multi_gpu:
#   enabled: true
#   strategy: single
#   main_device: 0
#
# =============================================================================
